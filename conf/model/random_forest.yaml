name: rf
n_estimators: 200
criterion: gini

# The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until
# all leaves contain less than min_samples_split samples. min_samples_split int or float, default=2
max_depth: null

# The number of features to consider when looking for the best split:
#    If int, then consider max_features features at each split.
#    If float, then max_features is a fraction and max(1, int(max_features * n_features_in_)) features are considered at each split.
#    If “sqrt”, then max_features=sqrt(n_features).
#    If “log2”, then max_features=log2(n_features).
#    If None, then max_features=n_features.
max_features: sqrt

# Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction
# in impurity. If None then unlimited number of leaf nodes.
max_leaf_nodes: null

# A node will be split if this split induces a decrease of the impurity greater than or equal
# to this value.
min_impurity_decrease: 0.0


# The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered
# if it leaves at least min_samples_leaf training samples in each of the left and right branches.
# This may have the effect of smoothing the model, especially in regression.
# If int, then consider min_samples_leaf as the minimum number.
# If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.
min_samples_leaf: 1

# The minimum number of samples required to split an internal node:
# If int, then consider min_samples_split as the minimum number.
# If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.
min_samples_split: 2

# The minimum weighted fraction of the sum total of weights (of all the input samples)
# required to be at a leaf node. Samples have equal weight when sample_weight is not provided.
min_weight_fraction_leaf: 0.0


# If bootstrap is True, the number of samples to draw from X to train each base estimator.

#    If None (default), then draw X.shape[0] samples.
#    If int, then draw max_samples samples.
#    If float, then draw max(round(n_samples * max_samples), 1) samples. Thus, max_samples should be in the interval (0.0, 1.0].
max_samples: null

# Whether to use out-of-bag samples to estimate the generalization score.
# By default, accuracy_score is used
oob_score: False


# Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that
# is smaller than ccp_alpha will be chosen. By default, no pruning is performed.
ccp_alpha: 0.0

# The “balanced” mode uses the values of y to automatically adjust weights inversely proportional
#   to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))
#
# The “balanced_subsample” mode is the same as “balanced” except that weights are computed based on the
#   bootstrap sample for every tree grown.
# Default: None
class_weight: balanced